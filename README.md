# MNIST Digits Classification â€” NumPy MLP ğŸ”¢ğŸ§ 

<p align="left">
  <img alt="MLP" src="https://img.shields.io/badge/MLP-Neural%20Network-ff69b4">
  <img alt="Backprop" src="https://img.shields.io/badge/FromScratch-Backprop-orange">
  <img alt="Status" src="https://img.shields.io/badge/Repo-Portfolio-blue">
</p>

**Author**: [NoÃ«lla Buti](https://github.com/NoellaButi)

---

## âœ¨ Overview
This project implements a **Multi-Layer Perceptron (MLP)** classifier for **MNIST digits** entirely in **NumPy**, with manual backpropagation, SGD + momentum, learning-rate decay, and optional L2 regularization & early stopping.

The goal: show **end-to-end understanding** of neural networks **without frameworks**, plus add **industry-style evaluation and artifacts**.

---

## ğŸ› ï¸ Workflow
1. ğŸ“¥ Load & normalize MNIST CSV data ([0.01, 1.0])  
2. ğŸ”¢ One-hot encode labels  
3. âš™ï¸ Xavier initialization  
4. â¡ï¸ Forward pass (sigmoid, softmax)  
5. ğŸ”„ Backpropagation (cross-entropy + L2)  
6. ğŸ“ˆ Train (SGD + momentum, LR decay, optional early stopping)  
7. âœ… Evaluate with confusion matrix, per-class metrics  
8. ğŸ” Industry pack: calibration (ECE), saliency maps, model card

---

## ğŸš¦ Results (Latest)
- **Hidden layer(s):** 1 Ã— 256 logistic  
- **Test accuracy:** **97.64%**  
- **Correct classifications:** 9,764  
- **Incorrect classifications:** 236  

ğŸ“Š Confusion matrix available in [`reports/confusion_matrix.png`](reports/confusion_matrix.png).

---

## ğŸ“ Repository Layout
- `notebooks/` â†’ Jupyter notebook with full step-by-step pipeline  
- `reports/` â†’ Metrics, plots, model card, run config  
- `artifacts/` â†’ Saved weights/biases (`.npz`)  
- `README.md` â†’ This overview

---

## ğŸ” Explainability
- **Classification report** (per-class precision/recall/F1)  
- **Most-confident mistakes** visualization  
- **Reliability diagram** + **ECE**  
- **Saliency maps**  

---

## ğŸ“œ License
MIT (see [LICENSE](LICENSE))

---
