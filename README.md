# MNIST Digits Classification â€” NumPy MLP ğŸ”¢ğŸ§   
Multi-Layer Perceptron (MLP) with Backpropagation **from scratch in NumPy**.

![Language](https://img.shields.io/badge/language-Python-blue.svg)
![Notebook](https://img.shields.io/badge/tool-Jupyter-orange.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![CI](https://github.com/NoellaButi/ai-mnist-mlp-fromscratch/actions/workflows/ci.yml/badge.svg?branch=main)

---

## âœ¨ Overview  
This project implements a **Multi-Layer Perceptron (MLP)** for MNIST digit classification using only **NumPy**.  
It demonstrates end-to-end understanding of neural networks without frameworks: manual forward pass, backpropagation, and training loop.  

Includes professional-style **evaluation artifacts**: confusion matrix, per-class metrics, calibration (ECE), saliency maps, and a minimal model card.

---

## ğŸ› ï¸ Workflow  
- Load & normalize MNIST CSV data â†’ `[0.01, 1.0]`  
- One-hot encode labels  
- Initialize weights with Xavier  
- Forward pass (sigmoid â†’ softmax)  
- Backpropagation (cross-entropy + L2 regularization)  
- Train with SGD + momentum + learning rate decay (optional early stopping)  
- Evaluate with accuracy, confusion matrix, precision/recall/F1  
- Save metrics + artifacts (plots, weights, configs)

---

## ğŸ“ Repository Layout  
```bash
ai-mnist-mlp-fromscratch/
â”œâ”€ notebooks/
â”‚  â””â”€ 01_mnist_mlp.ipynb        # full pipeline
â”œâ”€ reports/
â”‚  â”œâ”€ confusion_matrix.png
â”‚  â”œâ”€ train_history.json
â”‚  â”œâ”€ metrics.json
â”‚  â”œâ”€ model_card.json
â”‚  â””â”€ config.json
â”œâ”€ artifacts/
â”‚  â””â”€ mlp_weights_biases_1hidden.npz
â”œâ”€ data/                        # CSVs or download script
â”œâ”€ requirements.txt
â””â”€ README.md
```

## ğŸš¦ Quickstart
Run the Jupyter notebook:
```bash
jupyter notebook notebooks/01_mnist_mlp.ipynb
```

## ğŸ“Š Results (Held-Out Test Set)

| Metric                   | Value         |
|---------------------------|--------------:|
| Hidden Layers             | 1 Ã— 256 logistic |
| Test Accuracy             | **97.64%**    |
| Correct Classifications   | 9,764         |
| Incorrect Classifications | 236           |
| Test Loss (cross-entropy) | â‰ˆ 0.13â€“0.14   |


![Confusion Matrix](reports/confusion_matrix.png)


## ğŸ” Features
- Manual forward & backward pass in NumPy
- SGD with momentum + LR decay
- Early stopping and L2 regularization
- Evaluation: confusion matrix, precision/recall/F1
- Reliability diagram + Expected Calibration Error (ECE)
- Saliency maps for explainability
- Model card in `reports/`

## ğŸ”® Roadmap
- Add 2-hidden-layer experiment
- Replace sigmoid â†’ ReLU, try Adam optimizer
- Export ONNX + small inference demo

## ğŸ“œ License
MIT (see [LICENSE](LICENSE))

---
